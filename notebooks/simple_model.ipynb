{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calblanco/Desktop/Dev/NLP/NLP243/SemEval/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Who are the main characters in the movie Little Women?',\n",
       "  ['Qui sont les personnages principaux du film Les Quatre Filles du docteur March?',\n",
       "   'Qui sont les personnages principaux du film Les Quatre Filles du docteur March??']),\n",
       " ('Who are the main actors in the movie Miracle in Cell No. 7?',\n",
       "  ['Qui sont les acteurs principaux du film 7. Koğuştaki Mucize?']),\n",
       " ('How can Welsh onions be grown and harvested in home gardens?',\n",
       "  ['Comment la ciboule peut-elle être cultivé et récolté dans son jardin?',\n",
       "   'Comment peut-on cultiver et récolter la ciboule dans son potager?',\n",
       "   'Comment peut-on cultiver et récolter de la ciboule dans son jardin?']),\n",
       " ('What is the genre of A City of Sadness?',\n",
       "  ['À quel genre appartient La Cité des douleurs?',\n",
       "   'Quel est le genre du film La Cité des douleurs?',\n",
       "   'Quel est le genre de La Cité des douleurs?']),\n",
       " ('How would you describe The Princess and the Pea in one word?',\n",
       "  ['Comment peut-on décrire La Princesse au petit pois en un seul mot?',\n",
       "   'Comment décririez-vous La Princesse au petit pois en un seul mot?',\n",
       "   'Comment décririez-vous La Princesse au petit pois en un mot?']),\n",
       " ('What type of hill is Batu Caves made of?',\n",
       "  ['De quel type de colline les grottes de Batu sont-elles constituées?']),\n",
       " ('Where can you watch the TV series The Santa Clauses?',\n",
       "  ['Où pouvez-vous regarder la série télévisée Super Noël, la série?',\n",
       "   'Où regarder la série télévisée Super Noël, la série?']),\n",
       " ('Is the St Matthew Passion a sacred oratorio?',\n",
       "  ['La Passion selon Saint Matthieu est-elle un oratorio sacré?']),\n",
       " ('What was the premise of the TV series Family Matters?',\n",
       "  ['Quelle était la prémisse de la série La Vie de famille?',\n",
       "   'De quelle série La Vie de famille était-elle dérivée?',\n",
       "   'Quels étaient les prémices de la série télévisée La Vie de famille?']),\n",
       " ('What instrument is the suite Pictures at an Exhibition written for?',\n",
       "  [\"Pour quels instruments la suite Tableaux d'une exposition a-t-elle été écrite?\",\n",
       "   \"Pour quel instrument la suite Tableaux d'une exposition est-elle écrite?\"]),\n",
       " (\"What is the genre of Gulliver's Travels?\",\n",
       "  ['Quel est le genre du roman Les Voyages de Gulliver?',\n",
       "   'Quel est le genre du livre Les Voyages de Gulliver?',\n",
       "   'À quel genre littéraire Les Voyages de Gulliver appartiennent-ils?']),\n",
       " ('Which famous catchphrase is associated with the Simpson family?',\n",
       "  ['Quel slogan célèbre est associé à la Famille Simpson?',\n",
       "   'Quel slogan célèbre est associé à la famille Simpson?']),\n",
       " ('According to the Rabbins, what could Solomon learn from the Seal of Solomon?',\n",
       "  ['Selon les rabbins quel pouvoir le Sceau de Salomon pouvait-il lui conférer?',\n",
       "   'Selon les rabbins, que pouvait apprendre Salomon du Sceau de Salomon?']),\n",
       " ('How tall is the Ostankino Tower in Moscow?',\n",
       "  ['Quelle est la hauteur de la tour Ostankino à Moscou?']),\n",
       " ('Can you provide a brief summary of the movie The Mask of Zorro?',\n",
       "  ['Pouvez-vous donner un court résumé du film Le Masque de Zorro?',\n",
       "   'Pouvez-vous fournir un bref résumé du film Le Masque de Zorro?']),\n",
       " ('In which country did the events of Planet of the Apes take place?',\n",
       "  ['Dans quel pays les événements de La Planète des singes ont-ils eu lieu?']),\n",
       " ('Where is Viktor Yushchenko from?', [\"D'où vient Viktor Iouchtchenko?\"]),\n",
       " ('What genre does The Invisible Man belong to?',\n",
       "  [\"À quel genre appartient L'Homme invisible?\"]),\n",
       " ('Who eventually ended up with the Apple of Discord and why?',\n",
       "  [\"Qui s'est finalement retrouvé avec la pomme d'or de la discorde et pourquoi?\"]),\n",
       " ('Who is Mary, Princess Royal and Countess of Harewood?',\n",
       "  ['Qui est Mary du Royaume-Uni?'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_language(name:str):\n",
    "    '''return a languages src and target from the jsons located in /sample/'''\n",
    "    \n",
    "    data = pd.read_json(f'../sample/{name}.jsonl', lines=True)\n",
    "\n",
    "    return list(zip(data['source'].tolist(), data['target'].tolist()))\n",
    "\n",
    "\n",
    "a = load_language('fr_FR')\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comment peut-on culturer et récolter des oignons gallois dans les jardins domestiques?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"sup my dawg\"\n",
    "\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "encoded_hi = tokenizer(a[2][0], return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "    **encoded_hi,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"]\n",
    ")\n",
    "o = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_EN': 'en_XX',\n",
       " 'fr_FR': 'fr_XX',\n",
       " 'ar_AE': 'ar_AR',\n",
       " 'de_DE': 'de_DE',\n",
       " 'es_ES': 'es_XX',\n",
       " 'it_IT': 'it_IT',\n",
       " 'ja_JP': 'ja_XX',\n",
       " 'ko_KR': 'ko_KR',\n",
       " 'th_TH': 'th_TH',\n",
       " 'tr_TR': 'tr_TR',\n",
       " 'zh_TW': 'zh_CN'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the lookup table to get our langs into the format used by the model'''\n",
    "with open('../data/lookup_table.json', 'r') as f:\n",
    "    lookup = json.loads(f.read())\n",
    "    lookup = lookup['lookup']\n",
    "\n",
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu #using nltk bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = ['the', 'dog', 'crossed', 'the', 'road']\n",
    "ref = [['the', 'dog', 'crossed', 'the', 'road']] #simple example (reference must be list of list, hypothesis is a single list of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu(ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# per sample \n",
    "# translate into target language\n",
    "# compute bleu score \n",
    "# generate avg scores per language \n",
    "# generate report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qui sont les principaux acteurs du film Miracle in Cell No. 7 ?']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(lang:str, utterance:str):\n",
    "    '''Using the huggingface translator, translate the utterance from english to the target language'''\n",
    "    lang = lookup[lang]\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    encoded_hi = tokenizer(utterance, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded_hi,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[lang]\n",
    "    )\n",
    "    \n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "translate(\"fr_FR\", a[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rough_token(lang: str, utterance:str):\n",
    "    if lang == \"zh_TW\":\n",
    "        return list(utterance) #return the character level tokeniztion of the utterance\n",
    "    else:\n",
    "        return utterance.split(\" \") #return word level tokenization of utterance\n",
    "\n",
    "\n",
    "def compute_score(lang:str, reference:str, hypothesis:str): #calculate bleu score\n",
    "    refs = []\n",
    "    for ref in reference:\n",
    "        refs.append(rough_token(lang, ref))\n",
    "    \n",
    "    hyp = rough_token(lang, hypothesis)\n",
    "\n",
    "    return sentence_bleu(refs, hyp)\n",
    "\n",
    "def test(lang):\n",
    "    '''Take in a target language\n",
    "        load data\n",
    "        perform translations\n",
    "        calculate bleu score\n",
    "    '''\n",
    "    scores = []\n",
    "    results = {}\n",
    "    try:\n",
    "        samples = load_language(lang)\n",
    "        for sample in tqdm(samples, desc=f'Translating to {lang}'):\n",
    "            source, targets = sample\n",
    "            hypothesis = translate(lang, source)\n",
    "            score = compute_score(lang, targets, hypothesis[0])\n",
    "\n",
    "            results[source] = {}\n",
    "            results[source]['translation'] = hypothesis\n",
    "            results[source]['score'] = score\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        return (results, scores, sum(scores)/len(scores))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Encountered {e} while translating {lang}')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating to de_DE:   0%|          | 0/20 [00:00<?, ?it/s]/Users/calblanco/Desktop/Dev/NLP/NLP243/SemEval/venv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/calblanco/Desktop/Dev/NLP/NLP243/SemEval/venv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Translating to de_DE: 100%|██████████| 20/20 [00:51<00:00,  2.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'How tall is Saint Sophia Cathedral in Kyiv?': {'translation': ['Wie hoch ist die Kathedrale Saint Sophia in Kiew?'],\n",
       "   'score': 9.106239987484608e-155},\n",
       "  'How long was Mary of Burgundy married to Emperor Maximilian I?': {'translation': ['Wie lange war Maria von Burgund mit Kaiser Maximilian I. verheiratet?'],\n",
       "   'score': 1.0},\n",
       "  'What is the genre of The War of the Worlds?': {'translation': ['Was ist das Genre des Krieges der Welten?'],\n",
       "   'score': 0.4111336169005197},\n",
       "  'Where is Viktor Yushchenko from?': {'translation': ['Woher kommt Viktor Yuschenko?'],\n",
       "   'score': 9.53091075863908e-155},\n",
       "  'What was the original purpose of the Cathedral of Christ the Saviour?': {'translation': ['Was war der ursprüngliche Zweck der Kathedrale Christi der Erlöser?'],\n",
       "   'score': 0.5169731539571706},\n",
       "  'What is the genre of A City of Sadness?': {'translation': ['Was ist das Genre von A City of Sadness?'],\n",
       "   'score': 0.44632361378533286},\n",
       "  'How can Welsh onions be grown and harvested in home gardens?': {'translation': ['Wie können walisische Zwiebeln in heimischen Gärten angebaut und geerntet werden?'],\n",
       "   'score': 0.2998221389342337},\n",
       "  'What type of entity is the Galactic Republic?': {'translation': ['Welche Art von Entität ist die Galaktische Republik?'],\n",
       "   'score': 0.5},\n",
       "  \"Can baker's yeast be used in gluten-free baking?\": {'translation': ['Kann Backhefe in glutenfreiem Backen verwendet werden?'],\n",
       "   'score': 6.313993041533344e-78},\n",
       "  'How does neem oil benefit the body when consumed?': {'translation': ['Wie kommt neemöl beim Verzehr zum Nutzen für den Körper?'],\n",
       "   'score': 5.607773684948284e-78},\n",
       "  'What type of artwork is Do Androids Dream of Electric Sheep??': {'translation': ['Welche Art von Kunstwerk ist Do Androids Dream of Electric Sheep??'],\n",
       "   'score': 0.3508439695638686},\n",
       "  'Who are the main antagonistic forces in the World of Ice and Fire?': {'translation': ['Wer sind die wichtigsten antagonistischen Kräfte in der Welt von Eis und Feuer?'],\n",
       "   'score': 1.0},\n",
       "  'What is the significance of the Basilica of Our Lady of Guadalupe in Mexican culture?': {'translation': ['Welche Bedeutung hat die Basilika Unserer Lieben Frau von Guadalupe in der mexikanischen Kultur?'],\n",
       "   'score': 1.0},\n",
       "  \"Can Schrödinger's cat exist in both live and dead states simultaneously?\": {'translation': ['Kann Schrödingers Katze in Leben und Tod gleichzeitig existieren?'],\n",
       "   'score': 4.797597231912944e-78},\n",
       "  'What is the symbol of the Galactic Empire in the Star Wars universe?': {'translation': ['Was ist das Symbol des Galaktischen Reiches im Universum der Sternenkriege?'],\n",
       "   'score': 0.47987820666906633},\n",
       "  'Who were the main composers of medieval music?': {'translation': ['Wer waren die wichtigsten Komponisten der mittelalterlichen Musik?'],\n",
       "   'score': 4.797597231912944e-78},\n",
       "  'Who was Grand Duke Sergei Alexandrovich of Russia?': {'translation': ['Wer war Großherzog Sergei Alexandrowitsch von Russland?'],\n",
       "   'score': 0.488923022434901},\n",
       "  'Who played the titular character in the film The Man in the Iron Mask?': {'translation': ['Wer spielte die Titelfigur im Film Der Mann in der Eisenmaske?'],\n",
       "   'score': 0.8931539818068694},\n",
       "  'In which industry does Seishiro Kato work?': {'translation': ['In welcher Branche arbeitet Seishiro Kato?'],\n",
       "   'score': 1.0},\n",
       "  'Which religious denomination is Saint Catherine Monastery associated with?': {'translation': ['Mit welcher religiösen Konfession ist das Kloster Saint Catherine verbunden?'],\n",
       "   'score': 0.537284965911771}},\n",
       " [9.106239987484608e-155,\n",
       "  1.0,\n",
       "  0.4111336169005197,\n",
       "  9.53091075863908e-155,\n",
       "  0.5169731539571706,\n",
       "  0.44632361378533286,\n",
       "  0.2998221389342337,\n",
       "  0.5,\n",
       "  6.313993041533344e-78,\n",
       "  5.607773684948284e-78,\n",
       "  0.3508439695638686,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  4.797597231912944e-78,\n",
       "  0.47987820666906633,\n",
       "  4.797597231912944e-78,\n",
       "  0.488923022434901,\n",
       "  0.8931539818068694,\n",
       "  1.0,\n",
       "  0.537284965911771],\n",
       " 0.44621683349818675)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test('de_DE') #test on german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
