{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from database import query_by_alias, query_by_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9950732, 'word': 'Wolfgang Johnson', 'start': 11, 'end': 27}, {'entity_group': 'LOC', 'score': 0.9970612, 'word': 'Berlin', 'start': 42, 'end': 48}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\") #NER Model\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy='average')\n",
    "example = \"My name is Wolfgang Johnson and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Texas', 'LOC'), ('US', 'LOC')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Is Texas the largest state in US?'\n",
    "ner_results = nlp(example)\n",
    "\n",
    "\n",
    "\n",
    "def merge_entities(ner_output):\n",
    "    result = []\n",
    "\n",
    "    for item in ner_output:\n",
    "        name = item['word']\n",
    "        ent_type = item['entity_group']\n",
    "\n",
    "        result.append((name, ent_type))\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n",
    "merge_entities(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El fox bruno rápido salta sobre el perro escaso',\n",
       " 'La Inteligencia Artificial está revolucionando el mundo',\n",
       " 'Yo vivo en Alemania, Texas que es en los Estados Unidos',\n",
       " '¿Por qué sigue ocurriendo?',\n",
       " '¿Por qué los Estados Unidos siguen siendo algo que no es?',\n",
       " 'El Presidente (habla en inglés): De conformidad con el entendimiento alcanzado en las consultas previas del Consejo, consideraré que el Consejo de Seguridad desea concluir su examen del tema que figura en el orden del día.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "article_en = \"The head of the United Nations says there is no military solution in Syria\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\") \n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\") #Translator Model\n",
    "\n",
    "model_inputs = tokenizer(article_en, return_tensors=\"pt\")\n",
    "\n",
    "tr_table = { #Translation table to take in our expected language names and use them for facebooks model\n",
    "    'ar': 'ar_AR',\n",
    "    'de': 'de_DE',\n",
    "    'es': 'es_XX',\n",
    "    'fr': 'fr_XX',\n",
    "    'it': 'it_IT',\n",
    "    'ja': 'ja_XX'\n",
    "}\n",
    "\n",
    "def translate(source, lang): #translate wrapper \n",
    "    lang = tr_table[lang]\n",
    "    inputs = tokenizer(source, return_tensors=\"pt\", padding=True, truncation=True, max_length=200)\n",
    "    generated = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[lang])\n",
    "    return tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "batch_sources = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Artificial intelligence is revolutionizing the world\",\n",
    "    \"I live in Germany, Texas which is in the US\",\n",
    "    \"Why does it keep happening\",\n",
    "    \"Why does US keep becoming something it is not\",\n",
    "    \"Is Texas the biggest state in the US\",\n",
    "]\n",
    "\n",
    "\n",
    "translate(batch_sources, 'es') #example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Signal', 'ORG')\n",
      "('- Man', 'MISC')\n",
      "What kind of artwork is The Signal-Man?\n",
      "['¿Qué tipo de obra de arte es The Signal-Man?']\n",
      "¿Qué tipo de obra de arte es The Signal-Man?\n",
      "['¿Qué tipo de obra de arte es The Signal-Man?']\n",
      "¿Qué tipo de obra artistica es El guardavía?\n",
      "[('is', '<es> Islandia </es>'), ('Man', '<es> varón </es>')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What kind of artwork is The Signal-Man?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat_mask(source:str, lang:str):\n",
    "    res = nlp(source) #get entities \n",
    "    merged = merge_entities(res)\n",
    "\n",
    "    translated = []\n",
    "\n",
    "    for ent in merged:\n",
    "        ent_name = ent[0]\n",
    "        try:\n",
    "            q = query_by_name(ent_name, projections=lang) #try to get by name\n",
    "            if len(q) == 0:\n",
    "                q = query_by_alias(ent_name) #try to get by alias\n",
    "                q = query_by_name(q[0][0], projections=lang)\n",
    "            \n",
    "\n",
    "            ent_translated = q[0][0]\n",
    "            translated.append(f'<{lang}> {ent_translated} </{lang}>')\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    source = source + ' ' + ' '.join(translated)\n",
    "\n",
    "    return source    \n",
    "        \n",
    "    \n",
    "    \n",
    "concat_mask(example, 'ar')\n",
    "\n",
    "\n",
    "def substitution_mask(source:str, lang:str):\n",
    "    res = nlp(source) #get entities \n",
    "    merged = merge_entities(res)\n",
    "\n",
    "    translated = []\n",
    "\n",
    "    for ent in merged:\n",
    "        print(ent)\n",
    "        ent_name = ent[0]\n",
    "        try:\n",
    "            q = query_by_name(ent_name, projections=lang) #try to get by name\n",
    "            if len(q) == 0:\n",
    "                q = query_by_alias(ent_name) #try to get by alias\n",
    "                q = query_by_name(q[0][0], projections=lang)\n",
    "            \n",
    "\n",
    "            ent_translated = q[0][0]\n",
    "            translated.append((ent_name ,f'<{lang}> {ent_translated} </{lang}>'))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for name, translation in translated:\n",
    "        source = source.replace(name, f'{translation}')\n",
    "    \n",
    "\n",
    "    return source   \n",
    "\n",
    "def dumbass_mask(source:str, lang:str):\n",
    "    tokens = WordPunctTokenizer().tokenize(source)\n",
    "    merged = []\n",
    "    for i,_ in enumerate(tokens):\n",
    "        for j in range(i+1, len(tokens)):\n",
    "            merged.append(' '.join(tokens[i:j]))\n",
    "\n",
    "    translated = []\n",
    "    for ent in merged:\n",
    "        ent_name = ent\n",
    "        try:\n",
    "            q = query_by_name(ent_name, projections=lang) #try to get by name\n",
    "            if len(q) == 0:\n",
    "                q = query_by_alias(ent_name) #try to get by alias\n",
    "                q = query_by_name(q[0][0], projections=lang)\n",
    "            \n",
    "\n",
    "            ent_translated = q[0][0]\n",
    "            translated.append((ent_name ,f'<{lang}> {ent_translated} </{lang}>'))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(translated)\n",
    "    return source\n",
    "\n",
    "def remove_tokens(src:str, lang:str):\n",
    "    src = src.replace(f'<{lang}> ', '')\n",
    "    src = src.replace(f' </{lang}>', '')\n",
    "    return src\n",
    " \n",
    "example = \"What kind of artwork is The Signal-Man?\"\n",
    "answer = \"¿Qué tipo de obra artistica es El guardavía?\"\n",
    "lang = 'es'\n",
    "a = substitution_mask(example, lang)\n",
    "print(a)\n",
    "a = translate(a, lang)\n",
    "print(a)\n",
    "print(remove_tokens(a[0], lang))\n",
    "print(translate(example, lang))\n",
    "print(answer)\n",
    "\n",
    "dumbass_mask(example, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make jsons with the form\n",
    "# Predictions Format\n",
    "# {\n",
    "#   \"id\": \"Q627784_0\",\n",
    "#   \"prediction\": \"Come viene ricordato e onorato Yu il Grande nella storia e cultura cinese di oggi?\",\n",
    "# }\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_testing_data():\n",
    "    \"\"\"Load in all XC Testing data\"\"\"\n",
    "    path = f'../data/XC_test_data'\n",
    "    data = {}\n",
    "\n",
    "    for lang in os.listdir(path): #iterate over the files to load in dfs\n",
    "        file = f'{path}/{lang}/test.jsonl'\n",
    "        frame = pd.read_json(file, lines=True)\n",
    "        data[lang] = frame\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def run_test(name:str, mask_fn=lambda x: x, batch_size=32):\n",
    "    \"\"\"Run test over the XC dataset\n",
    "\n",
    "    Args:\n",
    "        - name (str): Name for the test\n",
    "        - mask_fn (fn(str) -> str): a Masking function for the source sentence [Default returns itself]\n",
    "    \"\"\"\n",
    "\n",
    "    data = load_testing_data()\n",
    "\n",
    "    try:\n",
    "        os.mkdir(f'../test_results/{name}')\n",
    "    except FileExistsError:\n",
    "        print('Path already exists')\n",
    "        \n",
    "    for lang, df in data.items(): #iterate over all dataframes\n",
    "\n",
    "        masked_sources = [] #Get masked version of sentences\n",
    "        row_ds = []\n",
    "        for i, row in  df.iterrows():\n",
    "            masked_source = mask_fn(row['source'])\n",
    "            masked_sources.append(masked_source)\n",
    "            row_ds.append(row['id'])\n",
    "\n",
    "        size = len(masked_sources) #size for determining batch count\n",
    "        \n",
    "        pairs = []\n",
    "        for batch in tqdm(range(round(size/batch_size)), desc=f'Translating batches for {lang}'): #batch \n",
    "            start = batch*batch_size\n",
    "            end = (batch+1) * batch_size if (batch+1) * batch_size < size else size\n",
    "\n",
    "            \n",
    "            sources = masked_sources[start:end]\n",
    "            #print(len(sources))\n",
    "            translated = translate(sources, lang)\n",
    "            #print(len(translated))\n",
    "\n",
    "            id_translation_pairs = list(zip(row_ds[start:end], translated))\n",
    "            pairs.extend(id_translation_pairs)\n",
    "\n",
    "        \n",
    "        with open(f'../test_results/{name}/{lang}.jsonl', 'w') as f:\n",
    "            shit = [json.dumps({'id': id, 'prediction': prediction})+\"\\n\" for id, prediction in pairs]\n",
    "            f.writelines(shit)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testing a <es> de </es> substitution jutsu <es> Shinigami fuckface </es>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'testing a substitution jutsu'\n",
    "\n",
    "ent_translation_pairs = [('a', '<es> de </es>'), ('jutsu', '<es> Shinigami fuckface </es>')]\n",
    "for name, trans_token in ent_translation_pairs:\n",
    "    s = s.replace(name, f'{name} {trans_token}')\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What']\n",
      "['What', 'kind']\n",
      "['What', 'kind', 'of']\n",
      "['What', 'kind', 'of', 'artwork']\n",
      "['What', 'kind', 'of', 'artwork', 'is']\n",
      "['What', 'kind', 'of', 'artwork', 'is', 'The']\n",
      "['What', 'kind', 'of', 'artwork', 'is', 'The', 'Signal']\n",
      "['What', 'kind', 'of', 'artwork', 'is', 'The', 'Signal', '-']\n",
      "['What', 'kind', 'of', 'artwork', 'is', 'The', 'Signal', '-', 'Man']\n",
      "['kind']\n",
      "['kind', 'of']\n",
      "['kind', 'of', 'artwork']\n",
      "['kind', 'of', 'artwork', 'is']\n",
      "['kind', 'of', 'artwork', 'is', 'The']\n",
      "['kind', 'of', 'artwork', 'is', 'The', 'Signal']\n",
      "['kind', 'of', 'artwork', 'is', 'The', 'Signal', '-']\n",
      "['kind', 'of', 'artwork', 'is', 'The', 'Signal', '-', 'Man']\n",
      "['of']\n",
      "['of', 'artwork']\n",
      "['of', 'artwork', 'is']\n",
      "['of', 'artwork', 'is', 'The']\n",
      "['of', 'artwork', 'is', 'The', 'Signal']\n",
      "['of', 'artwork', 'is', 'The', 'Signal', '-']\n",
      "['of', 'artwork', 'is', 'The', 'Signal', '-', 'Man']\n",
      "['artwork']\n",
      "['artwork', 'is']\n",
      "['artwork', 'is', 'The']\n",
      "['artwork', 'is', 'The', 'Signal']\n",
      "['artwork', 'is', 'The', 'Signal', '-']\n",
      "['artwork', 'is', 'The', 'Signal', '-', 'Man']\n",
      "['is']\n",
      "['is', 'The']\n",
      "['is', 'The', 'Signal']\n",
      "['is', 'The', 'Signal', '-']\n",
      "['is', 'The', 'Signal', '-', 'Man']\n",
      "['The']\n",
      "['The', 'Signal']\n",
      "['The', 'Signal', '-']\n",
      "['The', 'Signal', '-', 'Man']\n",
      "['Signal']\n",
      "['Signal', '-']\n",
      "['Signal', '-', 'Man']\n",
      "['-']\n",
      "['-', 'Man']\n",
      "['Man']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = WordPunctTokenizer().tokenize(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
